{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsoH_BtAOasm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, GRU, Dense\n",
        "\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"I love deep learning\",\n",
        "    \"deep learning is fun\",\n",
        "    \"I enjoy learning new things\",\n",
        "    \"machine learning is powerful\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram = token_list[:i+1]\n",
        "        input_sequences.append(n_gram)\n",
        "\n",
        "max_len = max(len(x) for x in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = Sequential([\n",
        "    Embedding(total_words, 10, input_length=X.shape[1]),\n",
        "    SimpleRNN(64),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_rnn.fit(X, y, epochs=200, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SwyksnkOge5",
        "outputId": "cd7e632c-9b84-41ae-f4b4-f29bccd1d649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f482498a2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential([\n",
        "    Embedding(total_words, 10, input_length=X.shape[1]),\n",
        "    LSTM(64),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.fit(X, y, epochs=200, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veY7qqemOnBg",
        "outputId": "215bacfa-18ca-427b-e630-aaa90fbef86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f48247b82d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bilstm = Sequential([\n",
        "    Embedding(total_words, 10, input_length=X.shape[1]),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model_bilstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_bilstm.fit(X, y, epochs=200, verbose=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvxwfkR1OtRY",
        "outputId": "fe649e2e-3394-49cb-d78c-e3cc12f1fd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f4821837e90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = Sequential([\n",
        "    Embedding(total_words, 10, input_length=X.shape[1]),\n",
        "    GRU(64),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model_gru.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_gru.fit(X, y, epochs=200, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y29_4A9PC5l",
        "outputId": "8bc89e84-14a5-4ea5-e53c-b9601d4cef90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f481b41b110>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Attention, Input, Concatenate, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(X.shape[1],))\n",
        "x = Embedding(total_words, 10)(inputs)\n",
        "lstm_out = LSTM(64, return_sequences=True)(x)\n",
        "attn = Attention()([lstm_out, lstm_out])\n",
        "x = Lambda(lambda x: tf.reduce_mean(x, axis=1))(attn)\n",
        "output = Dense(total_words, activation='softmax')(x)\n",
        "\n",
        "model_attention = Model(inputs, output)\n",
        "model_attention.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_attention.fit(X, y, epochs=200, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOa0hurLPHq2",
        "outputId": "6c5013d3-5e62-4012-8cce-a7dc703c256f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f4821491450>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, tokenizer, text, max_len):\n",
        "    token_seq = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_seq = pad_sequences([token_seq], maxlen=max_len-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_seq, verbose=0)\n",
        "    predicted_id = np.argmax(predicted_probs)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_id:\n",
        "            return word"
      ],
      "metadata": {
        "id": "BXGtTpy3PMXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next_word(model_lstm, tokenizer, \"I love\", max_len))\n",
        "print(predict_next_word(model_bilstm, tokenizer, \"I love\", max_len))\n",
        "print(predict_next_word(model_rnn, tokenizer, \"I love\", max_len))\n",
        "print(predict_next_word(model_attention, tokenizer, \"I love\", max_len))\n",
        "print(predict_next_word(model_gru, tokenizer, \"I love\", max_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIK7_FSbPiQt",
        "outputId": "2149dfa1-1440-4ce7-af8c-cc2ceebc4b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine\n",
            "deep\n",
            "machine\n",
            "learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f48197af380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(model, tokenizer, seed_text, max_len, n_words):\n",
        "    output_text = seed_text\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        token_seq = tokenizer.texts_to_sequences([output_text])[0]\n",
        "        token_seq = pad_sequences([token_seq], maxlen=max_len-1, padding='pre')\n",
        "\n",
        "        predicted_probs = model.predict(token_seq, verbose=0)\n",
        "        predicted_id = np.argmax(predicted_probs)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_id:\n",
        "                output_text += \" \" + word\n",
        "                break\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "zk9677gYPkL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = generate_sentence(model_bilstm, tokenizer, seed_text=\"I love\", max_len=max_len, n_words=7)\n",
        "print(sentence)\n",
        "\n",
        "sentence = generate_sentence(model_lstm, tokenizer, seed_text=\"I love\", max_len=max_len, n_words=7)\n",
        "print(sentence)\n",
        "\n",
        "sentence = generate_sentence(model_attention, tokenizer, seed_text=\"I love\", max_len=max_len, n_words=7)\n",
        "print(sentence)\n",
        "\n",
        "sentence = generate_sentence(model_gru, tokenizer, seed_text=\"I love\", max_len=max_len, n_words=7)\n",
        "print(sentence)\n",
        "\n",
        "sentence = generate_sentence(model_rnn, tokenizer, seed_text=\"I love\", max_len=max_len, n_words=7)\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvQKq4upQSaX",
        "outputId": "c26b3ff4-082e-4991-e900-9b4c4a8c650f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love deep learning things new new things learning\n",
            "I love machine learning new fun things things things\n",
            "I love learning is powerful things things things powerful\n",
            "I love deep learning new things things things powerful\n",
            "I love machine learning things powerful love love love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_8x1mLDQUkA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}